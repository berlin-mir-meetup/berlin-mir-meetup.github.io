## Important Announcements

Next meetup: Monday, September 18, 2017

## Links

- [website on meetup.com](https://www.meetup.com/Berlin-Music-Information-Retrieval-Meetup/)

<!-- ## Slides from Past Meetups -->

## Events

### 2017-10-16: Deep Learning for Music Recommendation and Generation

Monday, October 16, 2017

7:15 PM, hosted by [SoundCloud](https://www.soundcloud.com) -
Rheinsberger Str. 76/77 10115, Berlin
[map](https://maps.google.com/maps?f=q&hl=en&q=Rheinsberger+Str.+76%2F77+10115%2C+Berlin%2C+de)

#### Sander Dieleman: Deep Learning for Music Recommendation and Generation

The advent of deep learning has made it possible to extract high-level
information from perceptual signals without having to specify manually
and explicitly how to obtain it; instead, this can be learned from
examples. This creates opportunities for automated content analysis of
musical audio signals. In this talk, I will discuss how deep learning
techniques can be used for audio-based music recommendation. I will
also discuss my ongoing work on music generation in the raw waveform
domain with WaveNet.

Sander Dieleman is a Research Scientist at DeepMind in London, UK,
where he has worked on the development of AlphaGo and WaveNet. He was
previously a PhD student at Ghent University, where he conducted
research on feature learning and deep learning techniques for learning
hierarchical representations of musical audio signals. During his PhD
he also developed the Theano-based deep learning library Lasagne and
won solo and team gold medals respectively in Kaggle's "Galaxy Zoo"
competition and the first National Data Science Bowl. In the summer of
2014, he interned at Spotify in New York, where he worked on
implementing audio-based music recommendation using deep learning on
an industrial scale.


### 2017-09-18: Phase Reconstruction from Magnitude Spectrograms

Monday, September 18, 2017

6:30 PM, hosted by [Native Instruments](https://www.native-instruments.com/) -
Schlesische Straße 29-30, 10997 Berlin
[map](https://maps.google.com/maps?f=q&hl=en&q=Schlesische+Stra%C3%9Fe+29-30%2C+10997+Berlin%2C+de)


#### Christian Dittmar: Phase Reconstruction from Magnitude Spectrograms

Phase reconstruction from magnitude spectrograms is an important
pre-requisite for signal reconstruction in many audio processing
applications. Among others, proper phase reconstruction can be
beneficial for audio coding, speech enhancement, and source
separation, i.e., whenever we process audio signals in the Short-time
Fourier Transform domain. This talk covers the most relevant
techniques, namely Phase-locked Vocoder, Phase Gradient Integration,
Convex Optimization, and Iterative Phase Reconstruction, and discusses
their advantages and drawbacks. Furthermore, I will point out some
relationships between different methods and how relevant ideas spread
across from distinct research directions. I will conclude this talk
with some recommendations for the practical use of the investigated
algorithms, especially focusing on music source separation.

Christian Dittmar received the Diploma degree in electrical
engineering from the Jena University of Applied Sciences, Jena,
Germany, in 2003. Since summer 2014, he has been working toward the
Ph.D. degree in the research group of M. Müller, International Audio
Laboratories Erlangen, Germany. Before that, he had worked at the
Fraunhofer Institute for Digital Media Technology (IDMT), Ilmenau,
Germany. The institute is headed by Prof. K. Brandenburg, one of the
co-inventors of the MP3 audio compression format. From 2006 to 2014,
Christian Dittmar was the Head of the Semantic Music Technology
Research Group, IDMT. Since 2012, he has been also the CEO and
co-founder of the music technology start-up Songquito. In 2014, he
received the AES Citation Award for his involvement in organizing the
42th and 53th AES International Conference on Semantic Audio. In 2014,
he was nominated as one of the 39 visionaries of the digital society
in Germany by the Gesellschaft für Informatik. During the 2015
HAMR@ISMIR hackathon, he received the Best Hack Award. He authored and
coauthored a number of peer-reviewed papers on music information
retrieval topics. His recent research interests include music
information retrieval, audio signal processing, and music education
applications.


### 2017-07-17: Corpora for MIR Research & Computational Music Theory

Monday, July 17, 2017

7:30 PM, hosted by [CRCLR](https://crclr.org) -
[map](https://maps.google.com/maps?f=q&hl=en&q=Rollbergstrasse+26%2C+12053+Neuk%C3%B6lln%2C+Berlin%2C+de)

this meetup is supported by [Aitokaiku](http://www.aitokaiku.com)

#### Xavier Serra: Creating and maintaining corpora for MIR research

One of the biggest bottlenecks for the advancement of the research in
MIR is the lack of very large and openly available corpora of music
data, corpora with which to train and test machine learning models. In
this presentation, I want to talk about this issue and about the
initiatives in which my group is involved to tackle it. I will cover
freesound.org and the recent effort to create
the [FreesoundDataset](https://datasets.freesound.org). I will also
describe our efforts in collaborating with MusicBrainz to create
AcousticBrainz.org, a framework to crowdsource acoustic information
for music tracks. Finally, I will talk about our research in CompMusic
to develop [Dunya](http://dunya.compmusic.upf.edu), which comprises
corpora of several music repertoires plus software tools, for the
purpose of musicological research.

Xavier Serra is Associate Professor of the Department of Information
and Communication Technologies and Director of the Music Technology
Group at the Universitat Pompeu Fabra in Barcelona. After a
multidisciplinary academic education, he obtained a PhD in Computer
Music from Stanford University in 1989 with a dissertation on the
spectral processing of musical sounds that is considered a key
reference in the field. His research interests cover the computational
analysis, description, and synthesis of sound and music signals, with
a balance between basic and applied research and approaches from both
scientific/technological and humanistic/artistic
disciplines. Dr. Serra is very active in fields of Audio Signal
Processing, Sound and Music Computing, Music Information Retrieval and
Computational Musicology at the local and international levels, being
involved in the editorial board of a number of journals and
conferences and giving lectures on current and future challenges of
these fields. He was awarded an Advanced Grant from the European
Research Council to carry out the project CompMusic aimed at promoting
multicultural approaches in music information research. More info:
https://www.upf.edu/web/xavier-serra


#### Ryan Groves: Computational Music Theory: Working with Symbolic Data

This talk will present an overview of methods that look to extract
information from symbolic representations (such as digital
scores). The tasks discussed will include pattern recognition (e.g.,
melodic phrase detection), harmonic sequence analysis and melodic
reduction.

Ryan received his B.S. in Computer Science from UCLA, and continued on
to complete a Master's in Music Technology from McGill University. As
the former Director of R&D for Zya, he developed a musical messenger
app that automatically sings your texts, called Ditty. Ditty won the
Best Music App of 2015 by the Appy Awards. In 2016, his research in
computational music theory was awarded the Best Paper of the most
prominent music technology conference, ISMIR. With his new venture,
Melodrive, he and his co-founding team of two PhDs in Music and AI are
looking to build the world's best artificially intelligent composer,
and to change the way music is experienced in video games and virtual
environments.



### 2017-06-26: Source separation and Sample Detection

Monday, June 26, 2017

6:30 PM, hosted by [Ableton](https://www.ableton.com)
[Schönhauser Allee 6-7, Berlin](https://www.google.com/maps?f=q&hl=en&q=Sch%C3%B6nhauser+Allee+6-7,+Berlin,+de)


For the fourth Berlin MIR meetup, we're happy to announce the
following speakers:

#### Gerald Schuller: Multi-Channel Source Separation using Independent Component Analysis

Gerald Schuller is a professor at Technical University Ilmenau, where
he heads the Applied Media Systems group and is associated with the
Fraunhofer Institute for Digital Media Technology (IDMT). His research
focusses on audio and signal processing, in particular audio and
speech coding, filter banks, music information retrieval, and source
separation. Gerald will present recent works on multi-channel source
separation using independent component analysis (ICA).

#### Alexander Lerch: Drum Transcription and Sample Detection with Non-Negative Matrix Factorization

Non-Negative Matrix Factorization (NMF) is a popular and comparably
simple technique that has been successfully applied to MIR tasks such
as pitch transcription and source separation. In this talk, we will
discuss applying NMF to the task of transcribing drum events from
polyphonic mixtures and to the task of detecting a snippet of a song
(sample) in a new mix.

Alexander Lerch is an assistant professor at Georgia Tech University
and co-founder of zplane development, Berlin. Previously he was a
lecturer at the audio communications department of the Technical
University Berlin. His research focusses on music information
retrieval and digital signal processing.




### 2017-04-24: Harmonic Mixing & Rhythmic Similarity

Monday, April 24, 2017

7:30 PM, hosted by [Audio Communication Group of TU Berlin](http://www.ak.tu-berlin.de/menue/fachgebiet_audiokommunikation/parameter/en/)

[Marchstraße 8, 10587 Berlin, Berlin](https://maps.google.com/maps?f=q&hl=en&q=Marchstra%C3%9Fe+8%2C+10587+Berlin%2C+Berlin%2C+de) - [lab](http://www.hybrid-plattform.org/ueber-uns/hybrid-lab/)

For the second edition of the Music Information Retrieval meetup, the
Audio Communication Group of TU Berlin is hosting us in the
HybridLab.

#### Roman Gebhardt: Psychoacoustic Approaches to Harmonic Mixing

Roman Gebhardt (TU Berlin) will give a talk on his works on harmonic
analysis of music based on psychoacoustic principles. He will present
a system for automatic harmonic adjustment of two or more pieces of
music that builds upon the theory of roughness. As a counterpart to
well-known beat synchronization, this motivates synchronization for
music mixing in the spectral domain to maximize the musical consonance
of a mix. He will as well give insight into a perceptually based
filtering process for chroma-analysis of musical audio that has proven
successful in key detection tasks and outline the idea of an extension
of classic key detection to root notes.

#### Nico Lehrbach: Measuring Rhythmic Similarity of Music Excerpts

Nico Lehrbach (TU Berlin) will present his work investigating the
rhythmic similarity estimation of short music excerpts. Rhythmic
similarity is seen as one component of general music
similarity. Derived from existing measures of music similarity like
the Rhythmic Patterns and the Onset Patterns, he presents a new
approach, the Bar Patterns. A survey was carried out in order to gain
a quantitative, empirical ground truth of the rhythmic similarity of
short music excerpts presented to the participants. These empirical
data indicate the general possibility of measuring rhythmic similarity
and is taken as a base to compare the algorithmic approaches.


### 2017-03-20: Kick-off with Meinard Müller

Monday, March 20, 2017

7:30 PM, hosted by [SoundCloud](http://soundcloud.com/)

[Rheinsberger Str. 76/77 10115, Berlin](https://www.google.de/maps/place/Rheinsberger+Str.+76,+10115+Berlin/@52.53684,13.3927413,17z/data=!3m1!4b1!4m5!3m4!1s0x47a851f11cea3617:0xf49c66be6f06d7c0!8m2!3d52.53684!4d13.39493?hl=en) -
Reception is on the second floor of the factory building

We are very pleased to have
[Meinard Müller](https://www.audiolabs-erlangen.de/fau/professor/mueller)
from the International AudioLabs Erlangen as our first speaker for the
MIR meetup. He will give an overview of the current state-of-the-art
in Music Information Retrieval and an outlook on future directions and
research problems. After the talk, there will be the opportunity for
personal conversations with him and some members of his research group
as well as representatives from various Berlin/Potsdam based
university groups and companies working on MIR.

#### Speaker

Meinard Müller is professor for Semantic Audio Processing at the
International Audio Laboratories Erlangen, Germany, a joint
institution of the Friedrich-Alexander-Universität
Erlangen-Nürnberg (FAU) and the Fraunhofer Institute for
Integrated Circuits IIS. His research interests include music
processing, music information retrieval, audio signal processing,
content-based multimedia, and motion retrieval.

[https://www.audiolabs-erlangen.de/fau/professor/mueller](https://www.audiolabs-erlangen.de/fau/professor/mueller)


## Contact Us

This Meetup is organized by a few people in the Berlin area, from both
industry and academia.

You can reach the group using the mailing list of our [google group](https://groups.google.com/forum/#!forum/berlin-mir-meetup).

<!-- list here? -->
